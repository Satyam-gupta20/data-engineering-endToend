# Data Engineering End-to-End
# Dataset â†’ Star Schema â†’ Data Marts â†’ Visualizations (Endâ€‘toâ€‘End in One Notebook)

An endâ€‘toâ€‘end **data engineering + analytics** walkthrough that takes a public dataset from **raw â†’ cleaned â†’ star schema (fact + dims) â†’ data quality checks â†’ business marts â†’ charts** using a single Jupyter notebook.

- **Tech**: Python, DuckDB (SQL), Pandas, Matplotlib
- **Architecture**: Medallion pattern (**Bronze â†’ Silver â†’ Gold â†’ QA â†’ Marts**)
- **Outcome**: Reusable template to model any tabular dataset into a star schema and publish BIâ€‘ready marts.

> âš ï¸ Note: This repo uses a public dataset; you can swap it for any CSV/Parquet or API source. No companyâ€‘specific context required.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data_engineering.ipynb      # Endâ€‘toâ€‘end notebook
â””â”€â”€ README.md                   # You are here
```


---

## ğŸ§ª What the Notebook Does

1. **Ingestion (Bronze)**  
   - Loads a public dataset (via `datasets` library) into a DataFrame.
   - Registers it in **DuckDB** to enable fast SQL over inâ€‘memory tables.

2. **Transformation (Silver)**  
   - Cleans & standardizes columns, normalizes repeated structures.

3. **Dimensional Modeling (Gold)**  
   - Builds a **star schema**: one **fact** table + multiple **dimension** tables.  
   - Enforces surrogate keys and referential integrity.

4. **Data Quality (QA)**  
   - Row counts, null checks, FK integrity checks, and valid domain constraints.
   - Notes on how this maps to **dbt tests** in production.

5. **Business Marts (Analytics Layer)**  
   - Derives curated aggregates/views for BI, APIs, or AI features.

6. **Visualizations**  
   - Produces charts from marts to tell the final story.

---

## â–¶ï¸ How to Run with Another Dataset

- Replace the ingestion cell with your source (CSV/Parquet/API). Example:
```python
import pandas as pd
df = pd.read_csv("your_data.csv")
# register to DuckDB
import duckdb as ddb
con = ddb.connect()
con.register("bronze_source", df)
```
- Update the **Silver** cleaning and **Gold** SQL to reflect your columns.
- Rerun QA + marts + charts â€” everything else stays the same.

---

## ğŸ§± Key Design Choices

- **DuckDB** for local SQL + analytics performance (fast, no server).
- **Medallion pattern** to mirror modern lakehouse practices.
- **Singleâ€‘notebook flow** to keep things reproducible and beginnerâ€‘friendly.
- **Clear separation** between modeling layers (Bronze/Silver/Gold/QA/Marts).

---

## ğŸ“Š Example Questions You Can Answer
- Trend of top categories over time
- Entityâ€‘level leaderboards and distributions
- Timeâ€‘toâ€‘X stats with percentiles
- Quality gates: are we missing keys? duplicated dimensions?

---

## ğŸ”„ Reproducibility Notes
- Randomness is avoided; all steps are deterministic.
- Charts are generated from marts to ensure parity with BI tools.

---

## ğŸ” Attribution & Ethics
- The included dataset reference is **public/open**.  
- Remove/replace any proper nouns if you plan to present this as a generic case study.
- Do **not** include any confidential or interviewâ€‘specific context.

---

## ğŸ§­ Roadmap / Niceâ€‘toâ€‘Haves (anyone interested to contribute?)
- [ ] dbt tests parity (schema + data tests)
- [ ] Export marts to Parquet/CSV for BI
- [ ] ER diagram autoâ€‘generation via `erdantic` or `sqlmesh`
- [ ] Optional Streamlit dashboard fed by marts


